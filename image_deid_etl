#!/usr/bin/env python
import argparse
import json
import logging.config
import os
import sys

from sqlalchemy.exc import IntegrityError

from etl.custom_etl import delete_acquisitions_by_modality, delete_sessions
from etl.custom_flywheel import inject_sidecar_metadata
from etl.database import create_schema, import_uuids_from_set, get_all_processed_uuids
from etl.exceptions import ImproperlyConfigured
from etl.main_pipeline import validate_info, run_deid
from etl.orthanc import get_orthanc_url, get_uuids, download_unpack_copy

FLYWHEEL_GROUP = os.getenv("FLYWHEEL_GROUP")
if FLYWHEEL_GROUP is None:
    raise ImproperlyConfigured(
        "You must supply a valid Flywheel group in FLYWHEEL_GROUP."
    )

# Configure Python's logging module. The Django project does a fantastic job explaining how logging works:
# https://docs.djangoproject.com/en/4.0/topics/logging/
logging.config.dictConfig(
    {
        "version": 1,
        "disable_existing_loggers": False,
        "handlers": {
            "console": {
                "class": "logging.StreamHandler",
            },
        },
        "root": {
            "handlers": ["console"],
            "level": os.getenv("IMAGE_DEID_ETL_LOG_LEVEL", "INFO"),
        },
    }
)

logger = logging.getLogger(__name__)


def initdb(args) -> int:
    logger.info("Initializing database schema...")
    create_schema()
    logger.info("Success!")

    return 0


def import_uuids(args) -> int:
    """
    This command is used for importing existing, processed UUIDs from a 1-D JSON
    file.
    """
    try:
        uuids_to_import = json.load(args.uuid_file)
        import_uuids_from_set(uuids_to_import)
        logger.info("Imported %d new UUIDs.", len(uuids_to_import))
        return 0
    except Exception as e:
        logger.error("Error: %r\nAre you trying to import already processed UUIDs?", e)
        return 1


def check(args) -> int:
    new_uuids, _, _ = get_uuids(get_orthanc_url(), get_all_processed_uuids(), "all")

    # There is no guarantee that these UUIDs will be in any particular order,
    # only that they are unprocessed.
    if args.limit:
        new_uuids = new_uuids[: args.limit]

    if args.raw:
        print(*new_uuids, sep="\n")
    else:
        if new_uuids:
            logger.info("%d new studies found on Orthanc.", len(new_uuids))

            # Useful for local development. Allows you to mark all new studies as
            # processed, so the ETL doesn't try to process anything.
            if args.mark_processed:
                import_uuids_from_set(set(new_uuids))
                logger.info('Marked %d new studies as "processed."', len(new_uuids))
        else:
            logger.info("No new UUIDs found on Orthanc.")

    return 0


def validate(args) -> int:
    local_path = f"{args.program}/{args.site}/"
    file_path = local_path + "files/"

    # Validate that we have all the right info/mapping
    logger.info("Generating subject mapping for validation.")
    validate_info(local_path, args.program, file_path)

    return 0


def run(args) -> int:
    local_path = f"{args.program}/{args.site}/"

    for uuid in args.uuid:
        download_unpack_copy(
            get_orthanc_url(),
            uuid,
            local_path + "DICOMs/",
            args.skip_modalities,
        )

    # Remove any acquisitions/sessions that we don't want to process.
    delete_acquisitions_by_modality(local_path + "DICOMs/", "OT")
    delete_acquisitions_by_modality(local_path + "DICOMs/", "SR")

    # Delete any "script" sessions
    delete_sessions(local_path + "DICOMs/", "script")
    delete_sessions(local_path + "DICOMs/", "Bone Scan")

    # Run conversion, de-id, quarantine suspicious files, and restructure output for upload.
    logger.info("Commencing de-identification process...")
    run_deid(local_path, args.program)

    logger.info('Uploading "safe" files to Flywheel...')
    upload2fw(args)

    logger.info("Injecting sidecar metadata...")
    add_fw_metadata(args)

    logger.info("DONE PROCESSING STUDIES")
    if os.path.exists(local_path + "NIfTIs_to_check/"):
        logger.info("There are files to check in: " + local_path + "NIfTIs_to_check/")
    if os.path.exists(local_path + "NIfTIs_short_json/"):
        logger.info("There are files to check in: " + local_path + "NIfTIs_short_json/")

    try:
        logger.info("Updating list of UUIDs...")
        import_uuids_from_set(args.uuid)
    except IntegrityError as error:
        logger.error(
            "Unable to mark %d UUID(s) as processed. The UUID(s) already exist in the database: %r",
            len(args.uuid),
            error,
        )

    return 0


def upload2fw(args) -> int:
    source_path = f"{args.program}/{args.site}/NIfTIs/"
    for fw_project in next(os.walk(source_path))[1]:  # for each project dir
        proj_path = os.path.join(source_path, fw_project)
        os.system(
            f"fw ingest folder --group {FLYWHEEL_GROUP} --project {fw_project} --skip-existing -y --quiet {proj_path}"
        )

    return 0


def add_fw_metadata(args) -> int:
    local_path = f"{args.program}/{args.site}/"

    inject_sidecar_metadata(FLYWHEEL_GROUP, local_path + "NIfTIs/")

    return 0


def s3_backup_niftis(args) -> int:
    local_path = f"{args.program}/{args.site}/"
    s3_path = f"s3://d3b-phi-data-prd/imaging/radiology/{args.program}/{args.site}/"

    return os.system("aws s3 sync " + local_path + "NIfTIs/ " + s3_path + "NIfTIs/")


def main() -> int:
    parser = argparse.ArgumentParser(
        description="A WIP tool to assist with reading DICOM images from Orthanc, conversion to anonymized NIfTI "
        "images, and uploading to Flywheel.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument(
        "--program",
        nargs="?",
        default="cbtn",
        choices=["cbtn", "corsica"],
        help="program namespace",
    )
    parser.add_argument(
        "--site",
        nargs="?",
        default="chop",
        help="site namespace",
    )
    subparsers = parser.add_subparsers()

    parser_initdb = subparsers.add_parser("initdb")
    parser_initdb.set_defaults(func=initdb)

    parser_import_uuids = subparsers.add_parser("importuuids")
    parser_import_uuids.add_argument(
        "file",
        type=argparse.FileType("r"),
        help="JSON file containing Orthanc UUIDs to process",
    )
    parser_import_uuids.set_defaults(func=import_uuids)

    parser_check = subparsers.add_parser("check")
    parser_check.add_argument(
        "-l",
        "--limit",
        type=int,
        help="only use the last NUM UUIDs, instead of all UUIDs",
    )
    parser_check.add_argument(
        "--mark-processed",
        action="store_true",
        help="mark all Orthanc UUIDs found as processed",
    )
    parser_check.add_argument(
        "-r",
        "--raw",
        action="store_true",
        help="returns a newline-delimited list of unprocessed UUIDs",
    )
    parser_check.set_defaults(func=check)

    parser_validate = subparsers.add_parser("validate", help="check sub/ses mapping")
    parser_validate.set_defaults(func=validate)

    parser_run = subparsers.add_parser(
        "run",
        help="download images and run deidentification",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser_run.add_argument(
        "--skip-modalities",
        nargs="*",
        default=["DX", "US"],
        help="space-delimited list of modalities to skip",
    )
    parser_run.add_argument(
        "uuid", nargs="+", help="space-delimited list of UUIDs to process"
    )
    parser_run.set_defaults(func=run)

    parser_upload2fw = subparsers.add_parser(
        "upload2fw",
        help="upload results to Flywheel, when complete",
    )
    parser_upload2fw.set_defaults(func=upload2fw)

    parser_add_fw_metadata = subparsers.add_parser(
        "add-fw-metadata", help="add metadata in JSON sidecars to NIfTIs on Flywheel"
    )
    parser_add_fw_metadata.set_defaults(func=add_fw_metadata)

    parser_s3_backup_niftis = subparsers.add_parser(
        "s3-backup-niftis", help="copies NIfTIs to S3"
    )
    parser_s3_backup_niftis.set_defaults(func=s3_backup_niftis)

    args = parser.parse_args()
    args.func(args)

    return 0


if __name__ == "__main__":
    sys.exit(main())
