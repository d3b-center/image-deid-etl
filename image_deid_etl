#!/usr/bin/env python
import argparse
import json
import logging.config
import os
import shutil
import sys

from etl.custom_etl import delete_acquisitions_by_modality, delete_sessions
from etl.custom_flywheel import inject_sidecar_metadata
from etl.database import create_schema, import_uuids_from_set, get_all_processed_uuids
from etl.exceptions import ImproperlyConfigured
from etl.main_pipeline import validate_info, run_deid
from etl.orthanc import get_orthanc_url, get_uuids, download_unpack_copy

FLYWHEEL_GROUP = os.getenv("FLYWHEEL_GROUP")
if FLYWHEEL_GROUP is None:
    raise ImproperlyConfigured(
        "You must supply a valid Flywheel group in FLYWHEEL_GROUP."
    )

# Configure Python's logging module. The Django project does a fantastic job explaining how logging works:
# https://docs.djangoproject.com/en/4.0/topics/logging/
logging.config.dictConfig(
    {
        "version": 1,
        "disable_existing_loggers": False,
        "handlers": {
            "console": {
                "class": "logging.StreamHandler",
            },
        },
        "root": {
            "handlers": ["console"],
            "level": os.getenv("IMAGE_DEID_ETL_LOG_LEVEL", "INFO"),
        },
    }
)

logger = logging.getLogger(__name__)


def initdb(args) -> int:
    logger.info("Initializing database schema...")
    create_schema()
    logger.info("Success!")

    return 0


def import_uuids(args) -> int:
    """
    This command is used for importing existing, processed UUIDs from a 1-D JSON
    file.
    """
    try:
        uuids_to_import = json.load(args.uuid_file)
        import_uuids_from_set(uuids_to_import)
        logger.info("Imported %d new UUIDs.", len(uuids_to_import))
        return 0
    except Exception as e:
        logger.error("Error: %r\nAre you trying to import already processed UUIDs?", e)
        return 1


def check(args) -> int:
    new_uuids, missing_list, input_df = get_uuids(
        get_orthanc_url(), get_all_processed_uuids(), "all"
    )
    if new_uuids:
        logger.info("%d new studies found on Orthanc.", len(new_uuids))

        # Useful for local development. Allows you to mark all new studies as
        # processed, so the ETL doesn't try to process anything.
        if args.mark_processed:
            import_uuids_from_set(set(new_uuids))
            logger.info("Marked %d new studies as \"processed.\"", len(new_uuids))
    else:
        logger.info("No new UUIDs found on Orthanc")

    return 0


def run_pipeline(args) -> int:
    s3_path = (
        "s3://d3b-phi-data-prd/imaging/radiology/"
        + args.program
        + "/"
        + args.site
        + "/"
    )
    s3_path_ims = "s3://d3b-phi-data-prd/imaging/radiology/images4dl/"
    local_path = args.program + "/" + args.site

    # ====== validate the inputs ======
    if local_path[-1:] != "/":
        local_path = local_path + "/"

    if s3_path[-1:] != "/":
        s3_path = s3_path + "/"

    file_path = local_path + "files/"

    # validate that we have all the right info/mapping
    if args.validate:
        logger.info("Generating subject mapping for validation.")
        validate_info(local_path, args.program, file_path)

    # run the pipeline
    if args.run_pipeline:
        ## ************** download studies from Orthanc **************
        max_studies = 40
        logger.info("Checking for new UUIDs.")
        orthanc_url = get_orthanc_url()
        new_uuids, missing_list, input_df = get_uuids(
            orthanc_url, get_all_processed_uuids(), "all"
        )
        if len(new_uuids) > max_studies:
            new_uuids = new_uuids[0 : (max_studies - 1)]
        ##  ************** if there are new uuids, download & prep files **************
        if new_uuids:
            logger.info(
                "Found new UUID(s) Orthanc, beginning to download %d new studies (max_studies = %d).",
                len(new_uuids),
                max_studies,
            )
            session_modality_to_skip = ["DX", "US"]  # DX=XR
            download_unpack_copy(
                orthanc_url,
                s3_path,
                new_uuids,
                local_path + "DICOMs/",
                session_modality_to_skip,
            )
            ## remove any acquisitions/sessions that we don't want to process
            delete_acquisitions_by_modality(local_path + "DICOMs/", "OT")
            delete_acquisitions_by_modality(local_path + "DICOMs/", "SR")
            delete_sessions(
                local_path + "DICOMs/", "script"
            )  # delete any "script" sessions
            delete_sessions(local_path + "DICOMs/", "Bone Scan")
            # delete_sessions_by_modality(local_path+'DICOMs/','XR') # delete X-rays
            # delete_sessions_by_modality(local_path+'DICOMs/','US') # delete ultrasounds
        else:
            logger.info("No UUIDs found in Orthanc.")

        logger.info("Commencing de-identification process...")
        # Run conversion, de-id, quarantine suspicious files, and restructure output for upload.
        run_deid(local_path, s3_path, args.program)

        logger.info('Uploading "safe" files to Flywheel...')
        os.system(
            f"fw import folder --group {FLYWHEEL_GROUP} --skip-existing -y {args.program}/{args.site}/NIfTIs/"
        )
        logger.info("Injecting sidecar metadata...")
        inject_sidecar_metadata(FLYWHEEL_GROUP, local_path + "NIfTIs/")

        logger.info("Backing up NIfTIs to S3...")
        os.system("aws s3 sync " + local_path + "NIfTIs/ " + s3_path + "NIfTIs/")
        logger.info("DONE PROCESSING STUDIES")
        if os.path.exists(local_path + "NIfTIs_to_check/"):
            logger.info(
                "There are files to check in: " + local_path + "NIfTIs_to_check/"
            )
        if os.path.exists(local_path + "NIfTIs_short_json/"):
            logger.info(
                "There are files to check in: " + local_path + "NIfTIs_short_json/"
            )

        logger.info("Updating list of UUIDs...")
        import_uuids_from_set(set(new_uuids))

    # ====== upload to Flywheel ========================
    if args.upload2fw:
        os.system(
            f"fw import folder --group {FLYWHEEL_GROUP} --skip-existing -y {args.program}/{args.site}/NIfTIs/"
        )

    if args.add_fw_metadata:
        # assumes local structure == Flywheel structure
        inject_sidecar_metadata(FLYWHEEL_GROUP, local_path + "NIfTIs/")

    if args.s3_backup_niftis:
        os.system("aws s3 sync " + local_path + "NIfTIs/ " + s3_path + "NIfTIs/")

    if args.s3_backup_images:
        os.system("aws s3 sync " + local_path + "JPGs/ " + s3_path_ims)
        os.system("aws s3 sync " + local_path + "JPGs_3/ " + s3_path_ims)

    if args.delete_local:
        if os.path.exists(local_path + "DICOMs/"):
            shutil.rmtree(local_path + "DICOMs/")
            logger.info("Deleted from local: " + local_path + "DICOMs/")
        if os.path.exists(local_path + "NIfTIs/"):
            shutil.rmtree(local_path + "NIfTIs/")
            logger.info("Deleted from local: " + local_path + "NIfTIs/")
        if os.path.exists(local_path + "NIfTIs_short_json/"):
            shutil.rmtree(local_path + "NIfTIs_short_json/")
            logger.info("Deleted from local: " + local_path + "NIfTIs_short_json/")
        if os.path.exists(local_path + "NIfTIs_to_check/"):
            shutil.rmtree(local_path + "NIfTIs_to_check/")
            logger.info("Deleted from local: " + local_path + "NIfTIs_to_check/")
        if os.path.exists(local_path + "JPGs/"):
            shutil.rmtree(local_path + "JPGs/")
            logger.info("Deleted from local: " + local_path + "JPGs/")
        if os.path.exists(local_path + "JPGs_2/"):
            shutil.rmtree(local_path + "JPGs_2/")
            logger.info("Deleted from local: " + local_path + "JPGs_2/")
        if os.path.exists(local_path + "JPGs_3/"):
            shutil.rmtree(local_path + "JPGs_3/")
            logger.info("Deleted from local: " + local_path + "JPGs_3/")

    return 0


def main() -> int:
    parser = argparse.ArgumentParser(
        description="A WIP tool to assist with reading DICOM images from Orthanc, conversion to anonymized NIfTI "
        "images, and uploading to Flywheel.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    subparsers = parser.add_subparsers()

    parser_initdb = subparsers.add_parser("initdb")
    parser_initdb.set_defaults(func=initdb)

    parser_import_uuids = subparsers.add_parser("importuuids")
    parser_import_uuids.add_argument(
        "uuid_file",
        type=argparse.FileType("r"),
        help="JSON file containing Orthanc UUIDs to process.",
    )
    parser_import_uuids.set_defaults(func=import_uuids)

    parser_check = subparsers.add_parser("check")
    parser_check.add_argument(
        "--mark-processed",
        action="store_true",
        help="Mark all Orthanc UUIDs found as \"processed.\"",
    )
    parser_check.set_defaults(func=check)

    parser_run_pipeline = subparsers.add_parser("run_pipeline")
    parser_run_pipeline.add_argument(
        "--program",
        nargs="?",
        default="cbtn",
        choices=["cbtn", "corsica"],
        required=True,
        help="Program namespace.",
    )
    parser_run_pipeline.add_argument(
        "--site",
        nargs="?",
        default="chop",
        help="Site namespace.",
        required=True,
    )
    parser_run_pipeline.add_argument(
        "--run_pipeline",
        action="store_true",
        help='Run the pipeline & upload "safe" files.',
    )
    parser_run_pipeline.add_argument(
        "--delete_local",
        action="store_true",
        help="Delete files off EC2.",
    )
    parser_run_pipeline.add_argument(
        "--validate",
        action="store_true",
        help="Check sub/ses mapping.",
    )
    parser_run_pipeline.add_argument(
        "--upload2fw",
        action="store_true",
        help="Upload results to Flywheel, when complete.",
    )
    parser_run_pipeline.add_argument(
        "--add_fw_metadata",
        action="store_true",
        help="Add metadata in JSON sidecars to NIfTIs on Flywheel.",
    )
    parser_run_pipeline.add_argument(
        "--s3_backup_niftis",
        action="store_true",
        help="Copies NIfTIs to S3.",
    )
    parser_run_pipeline.add_argument(
        "--s3_backup_images",
        action="store_true",
        help="Copies JPGs to S3.",
    )
    parser_run_pipeline.set_defaults(func=run_pipeline)

    args = parser.parse_args()
    args.func(args)

    return 0


if __name__ == "__main__":
    sys.exit(main())
